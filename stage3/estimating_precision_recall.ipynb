{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep 1: steps to install the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install py_entitymatching\n",
    "!pip install scipy\n",
    "!pip install numpy\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [NEED MODIFICATION: Modify this cell to point to the file location]\n",
    "# Prep 2: enter the file location on your harddisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_a = 'table_a'\n",
    "table_b = 'table_b'\n",
    "candidate_set = 'candidate_set'\n",
    "prediction_set = 'prediction_set'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep 3: reading the files into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = pd.read_csv(table_a)\n",
    "dfb = pd.read_csv(table_b)\n",
    "dfc = pd.read_csv(candidate_set)\n",
    "dfp = pd.read_csv(prediction_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module: debug_blocker\n",
    "# Description: debug the blocking rule using the below script to ensure you are not dropping true matches\n",
    "# Note: You need to run Prep 1 and 2 in order to run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input format:\n",
    "# Format of table_a:\n",
    "# _id, attribute1, attribute2, ....., attributen\n",
    "\n",
    "# Format of table_b:\n",
    "# _id, attribute1, attribute2, ....., attributen\n",
    "\n",
    "# Format of candidate_set\n",
    "# A_id,B_id\n",
    "# where A_id is _id from table_a and B_id is the _id column value from table_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_entitymatching as em\n",
    "import pandas as pd\n",
    "\n",
    "def run_debug_blocker(table_a, table_b, table_a_key, table_b_key, candidate_set):\n",
    "    dfl = em.read_csv_metadata(table_a, key=table_a_key)\n",
    "    dfr = em.read_csv_metadata(table_b, key=table_b_key)\n",
    "\n",
    "    # reading the candidate set and adding key\n",
    "    dfcand = pd.read_csv(candidate_set)\n",
    "    dfcand.drop_duplicates(inplace=True)\n",
    "    dfcand.to_csv('cand_set_with_index.csv', index_label='id')\n",
    "\n",
    "    dfcset = em.read_csv_metadata('cand_set_with_index.csv', key='id', ltable=dfl, \n",
    "                                  rtable=dfr, fk_ltable='A_id', fk_rtable='B_id')\n",
    "\n",
    "    # running debug blocker to identify the records in A x B \\ C\n",
    "    debug_file = em.debug_blocker(dfcset, dfl, dfr)\n",
    "    \n",
    "    return debug_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_file = run_debug_blocker(table_a, table_b, '_id', '_id', candidate_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module: estimate_precision_recall\n",
    "# Description: the below code helps you get an estimation of P/R on the candidate set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from numpy import sqrt\n",
    "\n",
    "delta = .05\n",
    "Z = norm.ppf(1 - (delta / 2))\n",
    "\n",
    "def estimate_PR(labeled_pairs, reduced_cands, predicted_matches):\n",
    "    '''\n",
    "    labeled_pairs - a pandas dataframe with schema id1,id2,label\n",
    "                    Note label needs to be Boolean\n",
    "\n",
    "    reduced_cands - a pandas dataframe with schema id1,id2\n",
    "    predicted_matches - a pandas dataframe with schema id1,id2\n",
    "    \n",
    "    return:\n",
    "        ( (recall lower bound, recall upper bound), (precision lower bound, precision upper bound) )\n",
    "    '''\n",
    "\n",
    "    labeled_pairs.drop_duplicates(inplace=True)\n",
    "    labeled_pairs.columns = ['id1', 'id2', 'label']\n",
    "    reduced_cands.columns = ['id1', 'id2']\n",
    "    reduced_cand_set = set(zip(reduced_cands.id1, reduced_cands.id2))\n",
    "    predicted_matches = set(zip(predicted_matches.id1, predicted_matches.id2))\n",
    "    \n",
    "    # estimate the recall\n",
    "    # number of positives in the labeled sample\n",
    "    actual_pos = float(labeled_pairs.label.sum())\n",
    "    # the maximum number of postives in the candidate set\n",
    "    max_actual_pos = float(actual_pos + len(reduced_cand_set) - len(labeled_pairs))\n",
    "    \n",
    "    # true positives in the labeled sample\n",
    "    true_pos = float(labeled_pairs.apply(lambda x : (x['id1'], x['id2']) in predicted_matches and x['label'], axis=1).sum())\n",
    "    #estimated recall\n",
    "    recall = float(true_pos / actual_pos)\n",
    "\n",
    "    recall_error = Z * sqrt( ((recall * (1 - recall)) / (actual_pos)) * ((max_actual_pos - actual_pos) / (max_actual_pos - 1)) )\n",
    "\n",
    "\n",
    "    # estimate Precision\n",
    "    labeled_set  = set(zip(labeled_pairs.id1, labeled_pairs.id2))\n",
    "    predicted_pos = float(len(labeled_set & predicted_matches))\n",
    "    \n",
    "    predicted_pos_in_reduced_cand_set = float(len(reduced_cand_set & predicted_matches))\n",
    "    \n",
    "    alpha =  predicted_pos_in_reduced_cand_set / len(predicted_matches)\n",
    "    precision = alpha * (true_pos / predicted_pos)\n",
    "    \n",
    "    precision_error = alpha * Z * sqrt( ((precision * (1 - precision)) / predicted_pos) * (float((len(predicted_matches) - predicted_pos)) / (len(predicted_matches)  - 1)) )\n",
    "\n",
    "    return ((recall - recall_error, recall + recall_error),\n",
    "            (precision - precision_error, precision + precision_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3 Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density = 0.44\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "candidate_set_df = pd.read_csv('candidate_set')\n",
    "table_a_df = pd.read_csv('table_a', )\n",
    "table_a_df = table_a_df.rename(index=str, columns={\"_id\": \"A_id\"})\n",
    "table_b_df = pd.read_csv('table_b')\n",
    "table_b_df = table_b_df.rename(index=str, columns={\"_id\": \"B_id\"})\n",
    "first_join = pd.merge(candidate_set_df, table_a_df, on='A_id')\n",
    "result = pd.merge(first_join, table_b_df, on='B_id')\n",
    "\n",
    "# Extract a sample of 50 tuples randomly and label them\n",
    "sample1 = result.sample(n=50, random_state=4)\n",
    "\n",
    "# Read the labeled file back and compute the density\n",
    "labeled_sample1 = pd.read_csv('sample1_labeled')\n",
    "np = len(labeled_sample1[labeled_sample1.label == True])\n",
    "density = np*1.0/len(labeled_sample1)\n",
    "print 'Density =', density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since our density is more than 0.2 we can break here\n",
    "# Randomly samepl 350 more tuples. If we use the same seed we will get the first 50 elements we got before.\n",
    "final_sample = result.sample(n=400, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0.9690025475829874, 0.9995013894248866), (0.9559581611784236, 0.9820263349456073))\n"
     ]
    }
   ],
   "source": [
    "# read the labeled pairs file, i.e. the file with the labels\n",
    "labeled_pairs = pd.read_csv('final_labeled')\n",
    "print(estimate_PR(labeled_pairs, dfc, dfp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
